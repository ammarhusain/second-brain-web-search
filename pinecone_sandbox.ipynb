{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (0.26.5)\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from requests>=2.20->openai) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ammarh/opt/anaconda3/envs/pinecone/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.26.5\n",
      "    Uninstalling openai-0.26.5:\n",
      "      Successfully uninstalled openai-0.26.5\n",
      "Successfully installed openai-0.27.2\n"
     ]
    }
   ],
   "source": [
    "# %conda info\n",
    "# !pip install -qU openai pinecone-client datasets\n",
    "#!pip install markdown\n",
    "#!pip install --upgrade openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "openai.api_key = \"***\"\n",
    "embed_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8579602241516113\n"
     ]
    }
   ],
   "source": [
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='gpt-3.5-turbo', #'text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()\n",
    "\n",
    "def complete_gpt3_5(prompt):\n",
    "    res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful and clairvoyant savant and oracle who tries to best answer the users query using just the context they provide.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return res['choices'][0]['message']['content']\n",
    "\n",
    "def complete_gpt_4(prompt):\n",
    "    res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that elaborates on the users query using only the context they provide. If the context does not provide sufficient details for you to formulate an answer you politely let them know.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return res['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "query = \"Who was the first man to walk on the moon?\"\n",
    "import time\n",
    "start = time.time()\n",
    "complete_gpt3_5(query)\n",
    "end = time.time()\n",
    "print(start - end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, hashlib, fnmatch\n",
    "\n",
    "def search_image_files(filename, directory):\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            if f == filename:\n",
    "                ext = os.path.splitext(f)[1].lower()\n",
    "                if ext in ('.png', '.jpg', '.jpeg', '.gif'):\n",
    "                    return os.path.join(dirpath, f)\n",
    "    return None\n",
    "\n",
    "def get_all_attachments_in_text(note_string):\n",
    "    regex_pattern = r\"\\[\\[.*?\\]\\]|!\\[\\[.*?\\]\\]\"\n",
    "    strip_char = r\"\\[|\\]|!\"\n",
    "    matches = [re.sub(strip_char,'',match) for match in re.findall(regex_pattern,  note_string) ]\n",
    "    return matches\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Regular expression pattern for matching URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Remove URLs from the text\n",
    "    without_urls = re.sub(url_pattern, '', text)\n",
    "    return without_urls\n",
    "\n",
    "def remove_obsidian_links(text):\n",
    "    clean = re.compile('\\[\\[.*?\\]\\]|!\\[\\[.*?\\]\\]')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def parse_markdown_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    _, filename = os.path.split(file_path)\n",
    "    # Initialize a counter to keep track of how many '---'s we've seen\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        # If we've seen one '---' already and we've just seen another, return the rest of the lines\n",
    "        if line == '---\\n' and count == 1:\n",
    "            count += 1\n",
    "            lines = lines[i:]\n",
    "            break\n",
    "        # If we've just seen our first 'b', increment the count\n",
    "        elif line == '---\\n' and count == 0:\n",
    "            count += 1\n",
    "\n",
    "    print(count)\n",
    "    # insert the filename as first element\n",
    "    lines.insert(0, filename + \"\\n\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_markdown_files(directory):\n",
    "    markdown_files = []\n",
    "    for item in os.listdir(directory):\n",
    "        if item == \".obsidian\" or item == \"Daily\":\n",
    "            continue\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            markdown_files.extend(get_markdown_files(item_path))\n",
    "        elif item.endswith('.md'):\n",
    "            markdown_files.append(item_path)\n",
    "    return markdown_files\n",
    "\n",
    "def split_list(note_lines_list, word_threshold):\n",
    "    sublists = []\n",
    "    sublist = []\n",
    "    subtotal = 0\n",
    "    for sentence in note_lines_list:\n",
    "        sublist.append(sentence)\n",
    "        word_count = len(sentence.split())\n",
    "        if subtotal + word_count > word_threshold:\n",
    "            sublists.append(sublist)\n",
    "            sublist = []\n",
    "            subtotal = 0\n",
    "        subtotal += word_count\n",
    "    if sublist:\n",
    "        sublists.append(sublist)\n",
    "    return sublists\n",
    "\n",
    "\n",
    "def get_files_to_index(rootdir, searchstr='publish: true'):\n",
    "    files_to_index = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if fnmatch.fnmatch(file, '*.md') == True:\n",
    "                filepath = os.path.join(subdir, file)\n",
    "                if fnmatch.fnmatch(filepath,'*/.trash/*') == False:\n",
    "                    with open(filepath, 'r') as f:\n",
    "                        if searchstr in f.read():\n",
    "                            files_to_index.append(filepath)\n",
    "    return files_to_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['reinforcement learning.md\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " 'Courses :\\n',\n",
       " '[[xcs234 - reinforcement learning]]\\n',\n",
       " 'xcs229ii - [[3 - reinforcement learning]]\\n',\n",
       " '[[huggingface - deeprl]]\\n',\n",
       " '\\n',\n",
       " 'Random tinkering:\\n',\n",
       " '[Reinforcement Learning Tips and Tricks — Stable Baselines3 1.8.0a3 documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)\\n',\n",
       " '[Part 1: Key Concepts in RL — Spinning Up documentation](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\\n',\n",
       " '[Train a Mario-playing RL Agent — PyTorch Tutorials 1.12.1+cu102 documentation](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) : [Colab with tinkered code](https://colab.research.google.com/drive/1DtWyTd0P-ZPXIojn3RhxIRf9HszTQnmz?authuser=1#scrollTo=skXH2HhsW_Gn)\\n',\n",
       " '\\n',\n",
       " '- [x] #goal #_2023 Do the huggingface DeepRL course [[huggingface - deeprl]] ✅ 2023-02-07\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parse_markdown_file(\"/Users/ammarh/Documents/second-brain/command-center.md\")\n",
    "parse_markdown_file(\"/Users/ammarh/Documents/second-brain/3 - Resources/ai-ml/reinforcement learning.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7]\n",
      "[0, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "fo = [1,2,3,4,5,6,7]\n",
    "fo = fo[2:]\n",
    "print(fo)\n",
    "fo.insert(0,0)\n",
    "print(fo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list = get_files_to_index(\"/Users/ammarh/Documents/second-brain/\")\n",
    "len(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 329 chunks from 256 files\n"
     ]
    }
   ],
   "source": [
    "files_list = get_files_to_index(\"/Users/ammarh/Documents/second-brain/\")\n",
    "# files_list = get_markdown_files(\"/Users/ammarh/Documents/test-vault/.\")\n",
    "\n",
    "VECTOR_WORD_LIMIT = 800\n",
    "\n",
    "notes_data = []\n",
    "\n",
    "for file in files_list:\n",
    "    note_lines = parse_markdown_file(file)        \n",
    "    note_text_split = split_list(note_lines, VECTOR_WORD_LIMIT)\n",
    "    for i,note_text_split_snippet in enumerate(note_text_split):\n",
    "        note_snippet = \"\".join(note_text_split_snippet)\n",
    "        notes_data.append({\n",
    "            'uuid': hashlib.sha256((file + \"_^_\" + str(i)).encode()).hexdigest(),\n",
    "            'file': file,\n",
    "            'section': i,\n",
    "            'note': note_snippet\n",
    "        })\n",
    "\n",
    "print(f\"Adding {len(notes_data)} chunks from {len(files_list)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/ammarh/Documents/second-brain/2 - Areas/machine-learning/machine-learning.md', '/Users/ammarh/Documents/second-brain/3 - Resources/courses/xcs234 - reinforcement learning/xcs234 - reinforcement learning.md', '/Users/ammarh/Documents/second-brain/2 - Areas/machine-learning/shiny new models.md']\n"
     ]
    }
   ],
   "source": [
    "files = set([n['file'] for n in notes_data])\n",
    "a = [f for f in files if \"learning\" in f]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pinecone\n",
    "\n",
    "index_name = 'obsidian-second-brain'\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=\"***\",\n",
    "    environment=\"us-east1-gcp\"\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    print(\"creating new index\")\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['file']}\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "# index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(notes_data), batch_size)):\n",
    "    #print(f\"i - {i}\")\n",
    "    # find end of batch\n",
    "    i_end = min(len(notes_data), i+batch_size)\n",
    "    meta_batch = notes_data[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['uuid'] for x in meta_batch]\n",
    "    # get notes to encode\n",
    "    notes = [x['note'] for x in meta_batch]\n",
    "    # create embeddings (try-except added to avoid RateLimitError)\n",
    "    try:\n",
    "        res = openai.Embedding.create(input=notes, engine=embed_model)\n",
    "    except Exception as e:\n",
    "        # handle the exception by printing a message\n",
    "        print(f\"An exception occurred: \")#{repr(e)}\")\n",
    "        done = False\n",
    "        while not done:\n",
    "            #sleep(5)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=notes, engine=embed_model)\n",
    "                done = True\n",
    "            except Exception as e:\n",
    "                print(f\"Still getting an exception: {e} ... Passing\")\n",
    "                print(notes)\n",
    "                pass\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'uuid': x['uuid'],\n",
    "        'file': x['file'],\n",
    "        'note': x['note']\n",
    "    } for x in meta_batch]\n",
    "\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = openai.Embedding.create(\n",
    "    input=[\"how do I say hello in arabic?\"],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res['data'][0]['embedding']\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(xq, top_k=2, include_metadata=True)\n",
    "#print(res)\n",
    "filtered_matches = [x for x in res['matches'] if x['score'] > 0]\n",
    "#print(filtered_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 8750\n",
    "\n",
    "def retrieve(query, confidence=0.78):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=20, include_metadata=True)\n",
    "    filtered_matches = [x for x in res['matches'] if x['score'] > confidence]\n",
    "    return filtered_matches\n",
    "\n",
    "def retrieve_context(query, confidence=0.78):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=10, include_metadata=True)\n",
    "    filtered_matches = [x for x in res['matches'] if x['score'] > confidence]\n",
    "    contexts = [\n",
    "        \"File: \" + x['metadata']['file'] + \"\\nNote: \" + x['metadata']['note'] for x in filtered_matches\n",
    "    ] # the score is a variable that should be exposed to the user \n",
    "\n",
    "    if len(contexts) == 0:\n",
    "        prompt = \"I got nothing\"\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(0, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return (prompt, filtered_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "xcs234 - reinforcement learning.md\n",
      "# XCS234 Overview\n",
      "```ccard\n",
      "type: folder_brief_live\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "public-command-center.md\n",
      "# Technical Notes\n",
      "\n",
      "[[machine-learning]]\n",
      "\n",
      "# Course-work\n",
      "\n",
      "## Stanford\n",
      "\n",
      "### Ai Certification\n",
      "\n",
      "[[xcs224n - natural language processing]]\n",
      "[[xcs224u - natural language understanding]]\n",
      "[[xcs234 - reinforcement learning]]\n",
      "[[xcs229ii - machine learning]]\n",
      "\n",
      "### Product Management Certification\n",
      "\n",
      "[[xprod110 - product management]]\n",
      "[[xprod210 - mastering product management]]\n",
      "[[xprod120 - product costing]]\n",
      "\n",
      "# Hobbies\n",
      "\n",
      "[[scuba diving|scuba diving]]\n",
      "[[bucket-list |travel bucket list]]\n",
      "[[camper van]]\n",
      "\n",
      "Four Thousand Weeks.md\n",
      "# Four Thousand Weeks\n",
      "\n",
      "![rw-book-cover](https://m.media-amazon.com/images/I/71udc7ZQtVL._SY160.jpg)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[Oliver Burkeman]]\n",
      "Full Title: Four Thousand Weeks\n",
      "Category: #books\n",
      "Date Highlighted: [[2022-01-08-Saturday]]\n",
      "\n",
      "## Highlights\n",
      "- What makes it unbearable is your mistaken belief that it can be cured. —CHARLOTTE JOKO BECK ([Location 29](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=29))\n",
      "\t - Tags: #pink_readwise\n",
      "- The world is bursting with wonder, and yet it’s the rare productivity guru who seems to have considered the possibility that the ultimate point of all our frenetic doing might be to experience more of that wonder. ([Location 55](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=55))\n",
      "- Productivity is a trap. Becoming more efficient just makes you more rushed, and trying to clear the decks simply makes them fill up again faster. ([Location 163](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=163))\n",
      "- The real problem isn’t our limited time. The real problem—or so I hope to convince you—is that we’ve unwittingly inherited, and feel pressured to live by, a troublesome set of ideas about how to use our limited time, all of which are pretty much guaranteed to make things worse. ([Location 177](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=177))\n",
      "- Historians call this way of living “task orientation,” because the rhythms of life emerge organically from the tasks themselves, rather than from being lined up against an abstract timeline, the approach that has become second nature for us today. ([Location 219](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=219))\n",
      "- The fundamental problem is that this attitude toward time sets up a rigged game in which it’s impossible ever to feel as though you’re doing well enough. Instead of simply living our lives as they unfold in time—instead of just being time, you might say—it becomes difficult not to value each moment primarily according to its usefulness for some future goal, or for some future oasis of relaxation you hope to reach once your tasks are finally “out of the way.” ([Location 282](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=282))\n",
      "- If I could get enough work done, my subconscious had apparently concluded, I wouldn’t need to ask if it was all that healthy to be deriving so much of my sense of self-worth from work in the first place. And as long as I was always just on the cusp of mastering my time, I could avoid the thought that what life was really demanding from me might involve surrendering the craving for mastery and diving into the unknown instead. In my case, that turned out to mean committing to a long-term relationship and, later, making the decision with my wife to try to start a family—two things I’d notably failed to get done with any number of systems for getting things done. ([Location 323](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=323))\n",
      "- And most of our strategies for becoming more productive make things worse, because they’re really just ways of furthering the avoidance. After all, it’s painful to confront how limited your time is, because it means that tough choices are inevitable and that you won’t have time for all you once dreamed you might do. It’s also painful to accept your limited control over the time you do get: maybe you simply lack the stamina or talent or other resources to perform well in all the roles you feel you should. ([Location 343](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=343))\n",
      "- There is an alternative: the unfashionable but powerful notion of letting time use you, approaching life not as an opportunity to implement your predetermined plans for success but as a matter of responding to the needs of your place and your moment in history. ([Location 390](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=390))\n",
      "\t - Tags: #favorite_readwise\n",
      "- This notion that fulfillment might lie in embracing, rather than denying, our temporal limitations wouldn’t have surprised the philosophers of ancient Greece and Rome. They understood limitlessness to be the sole preserve of the gods; the noblest of human goals wasn’t to become godlike, but to be wholeheartedly human instead. ([Location 402](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=402))\n",
      "- This helps explain why stuffing your life with pleasurable activities so often proves less satisfying than you’d expect. It’s an attempt to devour the experiences the world has to offer, to feel like you’ve truly lived—but the world has an effectively infinite number of experiences to offer, so getting a handful of them under your belt brings you no closer to a sense of having feasted on life’s possibilities. Instead, you find yourself pitched straight back into the efficiency trap. The more wonderful experiences you succeed in having, the more additional wonderful experiences you start to feel you could have, or ought to have, on top of all those you’ve already had, with the result that the feeling of existential overwhelm gets worse. ([Location 539](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=539))\n",
      "- What’s needed instead in such situations, I gradually came to understand, is a kind of anti-skill: not the counterproductive strategy of trying to make yourself more efficient, but rather a willingness to resist such urges—to learn to stay with the anxiety of feeling overwhelmed, of not being on top of everything, without automatically responding by trying to fit more in. To approach your days in this fashion means, instead of clearing the decks, declining to clear the decks, focusing instead on what’s truly of greatest consequence while tolerating the discomfort of knowing that, as you do so, the decks will be filling up further, with emails and errands and other to-dos, many of which you may never get around to at all. ([Location 580](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=580))\n",
      "\n",
      "Bytes and Chips.md\n",
      "# Bytes and Chips\n",
      "\n",
      "![rw-book-cover](https://media.newyorker.com/photos/5c42513836b0283b1309925c/16:9/w_1280,c_limit/770404_ra611.jpg)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[Anthony Hiss]]\n",
      "Full Title: Bytes and Chips\n",
      "Category: #articles\n",
      "Document Tags: [ [[wkd]], ]\n",
      "URL: https://www.newyorker.com/magazine/1977/04/04/bytes-and-chips?utm_source=nl&utm_brand=tny&utm_mailing=TNY_Classics_Sunday_010422&utm_campaign=aud-dev&utm_medium=email&bxid=5f44071f42f18850782a533c&cndid=60476897&hasha=1e5ec51f2cab11499c35b78fbbd85aea&hashb=c015f1131e5effdf3ea5853638fc4f7110311871&hashc=9163bf8c03e4a0995979922f700d62775872afa5fe48f12dda1a925a230d2092&esrc=subscribe-page&mbid=CRMNYR062419\n",
      "Date Highlighted: [[2023-01-07-Saturday]]\n",
      "\n",
      "## Highlights\n",
      "- And in just a matter of weeks a couple of men in their twenties from Los Altos, California, the next town over from Mountain View, will start selling Apple II, which Helmers calls the first appliance computer—a fully assembled briefcase-size unit, with a large memory and a keyboard, that can play any number of computer games, draw pictures on your color TV, and operate like any other computer, using the TV as its display. Cost of Apple II: thirteen hundred dollars. ([View Highlight](https://read.readwise.io/read/01gp6h7bc2p5zva3h3br54mb8j))\n",
      "- “People will be able to do their home bookkeeping, they’ll be able to program their home computer to maintain a constant temperature in the apartment, and to start the coffeepot in the morning—eggs over easy will still be a human function—and then they’ll start hooking their machines in with their friends’ machines, and people will get on nets of computers, which could mean you could vote or buy something while sitting at your own computer keyboard. Games are the big thing at the moment, but what’s interesting to me is that people with little or no electronics background are getting into the field and, since they’ve never been instructed about what the computer can’t do, they’re already trying crazy things and coming up with completely unheard-of uses for computers.” ([View Highlight](https://read.readwise.io/read/01gp6h45vm454rjhmsmbransvp))\n",
      "- The breakthrough came when Intel, a Silicon Valley firm ... came up with significant improvements in the design of microcircuitry (circuits so small you need a microscope to see them). Intel started manufacturing silicon chips only two-tenths of an inch on a side which could contain up to twenty thousand transistors. When the chips first came out, in 1973, they were going for a hundred and fifty dollars apiece. Now they’re available for fifteen dollars apiece.\n",
      "\n",
      "AI Learns to Write Computer Code in ‘Stunning’ Advance.md\n",
      "# Ai Learns to Write Computer Code in ‘STUNNING’ Advance\n",
      "\n",
      "![rw-book-cover](https://www.science.org/do/10.1126/science.adg2088/abs/_20221208_on_aiwritescode.jpg)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[ByMatthew Hutson]]\n",
      "Full Title: AI Learns to Write Computer Code in ‘Stunning’ Advance\n",
      "Category: #articles\n",
      "Document Tags: [ [[aiml]], ]\n",
      "URL: https://www.science.org/content/article/ai-learns-write-computer-code-stunning-advance?utm_source=substack&utm_medium=email\n",
      "Date Highlighted: [[2022-12-21-Wednesday]]\n",
      "\n",
      "## Highlights\n",
      "- When presented with a fresh problem, AlphaCode generates candidate code solutions (in Python or C++) and filters out the bad ones. But whereas researchers had previously used models like Codex to generate tens or hundreds of candidates, DeepMind had AlphaCode generate up to more than 1 million.\n",
      "  To filter them, AlphaCode first keeps only the 1% of programs that pass test cases that accompany problems. ([View Highlight](https://read.readwise.io/read/01gmsmksrmtgwerb5gn9j8c1af))\n",
      "- After training, AlphaCode [solved about 34% of assigned problems](http://www.science.org/doi/10.1126/science.abq1158?adobe_mc=MCORGID%3D242B6472541199F70A4C98A6%2540AdobeOrg%7CTS%3D1670982433), DeepMind reports this week in *Science*. (On similar benchmarks, Codex achieved single-digit-percentage success.) ([View Highlight](https://read.readwise.io/read/01gmsmm98zd2jnwv06w9jv98an))\n",
      "- “It continues to be impressive how well machine-learning methods do when you scale them up,” he says. The results are “stunning,” adds Wojciech Zaremba, a co-founder of OpenAI and co-author of their Codex paper. ([View Highlight](https://read.readwise.io/read/01gmsmscnpq6w0fttav04f5aef))\n",
      "\n",
      "- “Trying to control the future is like trying to take the master carpenter’s place,” cautions one of the founding texts of Taoism, the Tao Te Ching, in a warning echoed several centuries later by the Buddhist scholar Geshe Shawopa, who gruffly commanded his students, “Do not rule over imaginary kingdoms of endlessly proliferating possibilities.” ([Location 1403](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=1403))\n",
      "\t - Tags: #favorite_readwise\n",
      "- We treat our plans as though they are a lasso, thrown from the present around the future, in order to bring it under our command. But all a plan is—all it could ever possibly be—is a present-moment statement of intent. It’s an expression of your current thoughts about how you’d ideally like to deploy your modest influence over the future. The future, of course, is under no obligation to comply. ([Location 1431](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=1431))\n",
      "- And it struck me, rather more uncomfortably, that the reason I’d been seeking all this advice in the first place was because this was my stance on life, too: that for as long as I could remember, my days had been spent striving for future outcomes—exam results, jobs, better exercise habits: the list went on and on—in the service of some notional time when life would run smoothly at last. ([Location 1500](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=1500))\n",
      "- The high achievers of Silicon Valley reminded Brown of herself in her days as an alcoholic. ([Location 1972](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=1972))\n",
      "- As the world gets faster and faster, we come to believe that our happiness, or our financial survival, depends on our being able to work and move and make things happen at superhuman speed. We grow anxious about not keeping up—so to quell the anxiety, to try to achieve the feeling that our lives are under control, we move faster. But this only generates an addictive spiral. We push ourselves harder to get rid of anxiety, but the result is actually more anxiety, because the faster we go, the clearer it becomes that we’ll never succeed in getting ourselves or the rest of the world to move as fast as we feel is necessary. ([Location 1991](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=1991))\n",
      "- We have to give up. You surrender to the reality that things just take the time they take, and that you can’t quiet your anxieties by working faster, because it isn’t within your power to force reality’s pace as much as you feel you need to, and because the faster you go, the faster you’ll feel you need to go. If you can let those fantasies crumble, Brown’s clients discovered, something unexpected happens, analogous to the alcoholic giving up his unrealistic craving for control in exchange for the gritty, down-to-earth, reality-confronting experience of recovery. ([Location 2012](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=2012))\n",
      "- When you finally face the truth that you can’t dictate how fast things go, you stop trying to outrun your anxiety, and your anxiety is transformed. Digging in to a challenging work project that can’t be hurried becomes not a trigger for stressful emotions but a bracing act of choice; giving a difficult novel the time it demands becomes a source of relish. “You cultivate an appreciation for endurance, hanging in, and putting the next foot forward,” Brown explains. You give up “demanding instant resolution, instant relief from discomfort and pain, and magical fixes.” You breathe a sigh of relief, and as you dive into life as it really is, in clear-eyed awareness of your limitations, you begin to acquire what has become the least fashionable but perhaps most consequential of superpowers: patience. ([Location 2018](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=2018))\n",
      "- Behind our urge to race through every obstacle or challenge, in an effort to get it “dealt with,” there’s usually the unspoken fantasy that you might one day finally reach the state of having no problems whatsoever. As a result, most of us treat the problems we encounter as doubly problematic: first because of whatever specific problem we’re facing; and second because we seem to believe, if only subconsciously, that we shouldn’t have problems at all. ([Location 2117](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=2117))\n",
      "- the most productive and successful among them generally made writing a smaller part of their [[top-of-mind-log|daily]] routine than the others, so that it was much more feasible to keep going with it day after day. They cultivated the patience to tolerate the fact that they probably wouldn’t be producing very much on any individual day, with the result that they produced much more over the long term. ([Location 2128](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=2128))\n",
      "- In many areas of life, there’s strong cultural pressure to strike out in a unique direction—to spurn the conventional options of getting married, or having kids, or remaining in your hometown, or taking an office job, in favor of something apparently more exciting and original. Yet if you always pursue the unconventional in this way, you deny yourself the possibility of experiencing those other, richer forms of uniqueness that are reserved for those with the patience to travel the well-trodden path first. ([Location 2161](https://readwise.io/to_kindle?action=open&asin=B08FGV64B1&location=2161))\n",
      "\n",
      "\t - Note: Lol\n",
      "- They all mentioned periods of religious doubt; the corruption of the world and their own hearts; the striking bottom and shattering of pride; and then finally the resurrection of self, a self alloyed to something larger. ([Location 4412](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=4412))\n",
      "- “It is this world, a world where cruise ships throw away more food in a day than most residents of Port-au-Prince see in a year, where white folks’ greed runs a world in need, apartheid in one hemisphere, apathy in another hemisphere … That’s the world! On which hope sits!” ([Location 4626](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=4626))\n",
      "- I began to imagine an unchanging rhythm of days, lived on firm soil where you could wake up each morning and know that all was how it had been yesterday, ([Location 4862](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=4862))\n",
      "- drew a series of circles around myself, with borders that shifted as time passed and faces changed but that nevertheless offered the illusion of control. An inner circle, where love was constant and claims unquestioned. Then a second circle, a realm of negotiated love, commitments freely chosen. And then a circle for colleagues, acquaintances; the cheerful gray-haired lady who rang up my groceries back in Chicago. Until the circle finally widened to embrace a nation or a race, or a particular moral course, and the commitments were no longer tied to a face or a name but were actually commitments I’d made to myself. ([Location 5131](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5131))\n",
      "- A part of me wished I could live up to the image that my new relatives imagined for me: a corporate lawyer, an American businessman, my hand poised on the spigot, ready to rain down like manna the largesse of the Western world. ([Location 5163](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5163))\n",
      "- Her restlessness, her independence, her constant willingness to project into the future—all of this struck the family as unnatural somehow. Unnatural … and un-African. ([Location 5173](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5173))\n",
      "- same perverse survivor’s guilt that I could expect to experience ([Location 5176](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5176))\n",
      "- So you have to draw the line somewhere. If everyone is family, no one is family. Your father, he never understood this, ([Location 5285](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5285))\n",
      "- An impression of loneliness that perhaps wasn’t true, perhaps was just a projection of my own heart, but that, either way, had made me want to run, just as, an ocean away, David had run, back into the marketplace and noisy streets, back into disorder and the laughter disorder produced, back into the sort of pain a boy could understand. ([Location 5334](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5334))\n",
      "- That’s how the next hour passed, with Ruth alternating between stories of my father’s failure and stories of Mark’s accomplishments. ([Location 5357](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=5357))\n",
      "- One only knows what one knows. Perhaps if I were young today, I would not have accepted these things. Perhaps I would only care about my feelings, and falling in love. But that’s not the world I was raised in. I only know ([Location 6331](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=6331))\n",
      "- what I have seen. What I have not seen doesn’t make my heart heavy.” ([Location 6332](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=6332))\n",
      "- The study of law can be disappointing at times, a matter of applying narrow rules and arcane procedure to an uncooperative reality; a sort of glorified accounting that serves to regulate the affairs of those who have power—and ([Location 6790](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=6790))\n",
      "- The law is also memory; the law also records a long-running conversation, a nation arguing with its conscience. ([Location 6793](https://readwise.io/to_kindle?action=open&asin=B000N2HCM4&location=6793))\n",
      "\n",
      "Feynman.md\n",
      "# Feynman\n",
      "\n",
      "![rw-book-cover](https://books.google.com/books/content?id=CnhBLgEACAAJ&printsec=frontcover&img=1&zoom=5&source=public)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[Jim Ottaviani]]\n",
      "Full Title: Feynman\n",
      "Category: #books\n",
      "Date Highlighted: [[2022-05-23-Monday]]\n",
      "\n",
      "## Highlights\n",
      "- IF EACH LETTER OF THE ENCYCLOPEDIA BRITANNICA TAKES 6 OR 7 BITS OF INFORMATION, WE WOULD NEED TO STORE ABOUT 10^15 BITS TO HAVE EVERYTHING IN EVERY BOOK IN THE WORLD. BUT IF FOR EACH BIT I ALLOW A CUBE OF 5X5X5 ATOMS, ALL OF THE INFORMATION IN ALL THE BOOKS IN THE WORLD CAN BE WRITTEN DOWN IN A CUBE OF MATERIAL ABOUT 1/200TH OF AN INCH WIDE. (Page 158)\n",
      "- FROM HIM I LEARNED WE CAN IMAGINE THAT THIS COMPLICATED ARRAY OF MOVING THINGS \"THE WORLD\"-IS SORTA LIKE A GREAT GAME PLAYED BY THE GODS, AND WE'RE OBSERVERS.\n",
      "\n",
      "Karl Marx, Yesterday and Today.md\n",
      "# Karl Marx, Yesterday and Today\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article1.be68295a7e40.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[newyorker.com]]\n",
      "Full Title: Karl Marx, Yesterday and Today\n",
      "Category: #articles\n",
      "URL: http://www.newyorker.com/magazine/2016/10/10/karl-marx-yesterday-and-today\n",
      "Date Highlighted: [[2021-11-08-Monday]]\n",
      "\n",
      "## Highlights\n",
      "- On or about February 24, 1848, a twenty-three-page pamphlet was published in London. Modern industry, it proclaimed, had revolutionized the world. It surpassed, in its accomplishments, all the great civilizations of the past—the Egyptian pyramids, the Roman aqueducts, the Gothic cathedrals. Its innovations—the railroad, the steamship, the telegraph—had unleashed fantastic productive forces. In the name of free trade, it had knocked down national boundaries, lowered prices, made the planet interdependent and cosmopolitan. Goods and ideas now circulated everywhere. ([View Highlight](https://instapaper.com/read/1458585946/17931639))\n",
      "- Hegel argued that history was the progress of humanity toward true freedom, by which he meant self-mastery and self-understanding, seeing the world without illusions—illusions that we ourselves have created. The Young Hegelians’ controversial example of this was the Christian God. (This is what Feuerbach wrote about.) We created God, and then pretended that God created us. We hypostatized our own concept and turned it into something “out there” whose commandments (which we made up) we struggle to understand and obey. We are supplicants to our own fiction. ([View Highlight](https://instapaper.com/read/1458585946/17931975))\n",
      "- What makes it hard to discard the tools we have objectified is the persistence of the ideologies that justify them, and which make what is only a human invention seem like “the way things are.” Undoing ideologies is the task of philosophy. Marx was a philosopher. ([View Highlight](https://instapaper.com/read/1458585946/17932005))\n",
      "- eleventh thesis on Feuerbach: the purpose of philosophy is to understand conditions in order to change them. Marx liked to say that when he read Hegel he found philosophy standing on its head, so he turned it over and placed it on its feet. Life is doing, not thinking. It is not enough to be the masters of our armchairs. ([View Highlight](https://instapaper.com/read/1458585946/17932342))\n",
      "- as anti-Communists used say about it ([View Highlight](https://instapaper.com/read/1458585946/17932377))\n",
      "\t - Note: Used to say\n",
      "- Human beings are naturally creative and sociable. A system that treats them as mechanical monads is inhumane. But the question is, How would a society without a division of labor produce sufficient goods to survive? Nobody will want to rear the cattle (or clean the barn); everyone will want to be the critic. (Believe me.) As Marx conceded, capitalism, for all its evils, had created abundance. He seems to have imagined that, somehow, all the features of the capitalist mode of production could be thrown aside and abundance would magically persist. ([View Highlight](https://instapaper.com/read/1458585946/17932417))\n",
      "- “Capital in the Twenty-first Century.” The book did for many twenty-first-century readers what Marx hoped “Capital” might do for nineteenth-century ones. It uses data to show us the real nature of social relations and, by doing that, forces us to rethink concepts that have come to seem natural and inevitable. One of these is the concept of the market, which is often imagined as a self-optimizing mechanism it is a mistake to interfere with, but which in fact, left to itself, continually increases inequality. Another concept, closely related, is meritocracy, which is often imagined as a guarantor of social mobility but which, Piketty argues, serves mainly to make economic winners feel virtuous. ([View Highlight](https://instapaper.com/read/1458585946/17932436))\n",
      "- We invented our social arrangements; we can alter them when they are working against us. There are no gods out there to strike us dead if we do. ([View Highlight](https://instapaper.com/read/1458585946/17932496))\n",
      "\n",
      "Whispers of A.I.’s Modular Future.md\n",
      "# Whispers of A.I.’s Modular Future\n",
      "\n",
      "![rw-book-cover](https://media.newyorker.com/photos/63d93e688b2aff35d30ef8e2/16:9/w_1280,c_limit/Somers_final.jpg)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[James Somers]]\n",
      "Full Title: Whispers of A.I.’s Modular Future\n",
      "Category: #articles\n",
      "URL: https://www.newyorker.com/tech/annals-of-technology/whispers-of-ais-modular-future?utm_medium=social&utm_brand=tny&mbid=social_facebook&utm_social-type=owned&utm_source=facebook\n",
      "Date Highlighted: [[2023-02-05-Sunday]]\n",
      "\n",
      "## Highlights\n",
      "- As Sutton put it in his 2019 essay, seventy years of A.I. research had revealed that “general methods that leverage computation are ultimately the most effective, and by a large margin.” Sutton called this “the bitter lesson”: it was bitter because there was something upsetting about the fact that packing more cleverness and technical arcana into your A.I. programs was not only inessential to progress but actually an impediment. ([View Highlight](https://read.readwise.io/read/01grh6ds5n2r1f1nn5z3y3rtzf))\n",
      "- Whisper’s story reveals a lot about the history of A.I. and where it’s going. When a piece of software is open-source, you can adapt it to your own ends—it’s a box of Legos instead of a fully formed toy—and software that’s flexible is remarkably enduring. ([View Highlight](https://read.readwise.io/read/01grh71pvpacnp0546k2phabh2))\n",
      "\n",
      "Gradient Update #24 Robotaxis in Beijing and a Multi-Task Visual Language Model.md\n",
      "# Gradient Update #24: Robotaxis in Beijing and a Multi-Task Visual Language Model\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png)\n",
      "\n",
      "# Metadata\n",
      "\n",
      "Author: [[thegradientpub.substack.com]]\n",
      "Full Title: Gradient Update #24: Robotaxis in Beijing and a Multi-Task Visual Language Model\n",
      "Category: #articles\n",
      "URL: https://thegradientpub.substack.com/p/gradient-update-24-robotaxis-in-beijing\n",
      "Date Highlighted: [[2022-07-01-Friday]]\n",
      "\n",
      "# Highlights\n",
      "- Flamingo integrates a powerful pretrained language model and vision model into a single framework, thus taking advantage of well-understood single modality foundation models. For  language, Flamingo uses a pretrained Chinchilla language model with 70 billion parameters. For vision, Flamingo uses a contrastively pretrained Normalizer Free ResNet (NFNet) with 435 million parameters. During training, the parameters of these two models are frozen. And once it’s complete, Flamingo can be directly adapted to vision tasks via simple few-shot learning without any additional task-specific finetuning.\n",
      "\n",
      "Four Years in Startups.md\n",
      "# Four Years in Startups\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[newyorker.com]]\n",
      "Full Title: Four Years in Startups\n",
      "Category: #articles\n",
      "URL: https://www.newyorker.com/magazine/2019/09/30/four-years-in-startups\n",
      "Date Highlighted: [[2021-09-30-Thursday]]\n",
      "\n",
      "## Highlights\n",
      "- I stuck to the narrative that working in analytics would be an experiment in separating my professional life from my personal life. Maybe I would start the short-story collection I had always wanted to write. Maybe I would take up pottery. I could learn to play the bass. I could have the sort of creative life that creative work would not sustain. It was easier to fabricate a romantic narrative than to admit that I was ambitious—that I wanted my life to pick up momentum.\n",
      "- Sometimes I would yearn for their sense of ownership and belonging—the easy identity, the all-consuming feeling of affiliation. And then I would remind myself, There but for the grace of God go I.\n",
      "- I would open a browser window and begin the day’s true work of toggling between tabs.Platforms designed to accommodate and harvest infinite data inspired an infinite scroll.\n",
      "- I knew, as I wandered through museums with friends and video-chatted with Ian, that I needed to leave the tech industry. I was no longer high on the energy of being around people who so easily satisfied their desires––on the feeling that everything was just within reach. The industry’s hubris and naïveté were beginning to grate; I had moral, political, and personal misgivings about Silicon Valley’s accelerating colonization of art, work, everyday life.\n",
      "\n",
      "Counsels and Maxims.md\n",
      "# Counsels and Maxims\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[Arthur Schopenhauer]]\n",
      "Full Title: Counsels and Maxims\n",
      "Category: #books\n",
      "Date Highlighted: [[2021-05-20-Thursday]]\n",
      "\n",
      "## Highlights\n",
      "\n",
      "### General Rules\n",
      "- Aristotle: *“not pleasure, but freedom from pain, is what the wise man will aim at”.*\n",
      "- For all pleasures are chimerical, and to mourn for having lost any of them is a frivolous, and even ridiculous proceeding.\n",
      "- In moments free from pain, our restless wishes present, as it were in a mirror, the image of a happiness that has no counterpart in reality, seducing us to follow it; in doing so we bring pain upon ourselves, and that is something undeniably real.\n",
      "  In seeking for these pleasures he encounters danger. He hunts for game that does not exist; and so he ends by suffering some very real and positive misfortunes.\n",
      "- To desire to get rid of an evil is a definite object, but to desire a better fortune than one has is blind folly.\n",
      "- For the safest way of not being very miserable is not to expect to be very happy\n",
      "- Persian poem:\n",
      "  Though from thy grasp all worldly things should flee,\n",
      "  Grieve not for them, for they are nothing worth:\n",
      "  And though a world in thy possession be,\n",
      "  Joy not, for worthless are the things of earth.\n",
      "- In their search for gold, the alchemists discovered other things - gunpowder, china, medicines, the laws of nature. There is a sense in which we are all alchemists.\n",
      "\n",
      "### Our relation to Ourselves\n",
      "- Simplicity, therefore, as far as it can be attained, and even monotony, in our manner of life, if it does not mean that we are bored, will contribute to happiness.\n",
      "- It is advisable to suspend mental work for a while, if circumstances happen which demand any degree of energy in affairs of practical nature.\n",
      "- A rule recommended by Pythagoras - to review, every night before going to sleep, what we have done during the day.\n",
      "- The love of life is at bottom only the fear of death.\n",
      "- It is the monotony of his own nature that makes a man find solitude intolerable.\n",
      "- Let me advise you, then, to form the habit of taking some of your solitude with you into society, to learn to be to some extent alone even though you are in company - not to say at once what you think, and on the other hand, not to attach too precise a meaning to what others say; Rather not to expect much of them either morally or intellectually, and to strengthen yourself in the feeling of indifference to their opinion, which is the surest way of always practicing a praise worthy toleration. If you do that, you will not live so much with other people, though you may appear to move amongst them. Your relation to them will be of a purely objective character.\n",
      "- Society is in this respect like a fire - the wise man warming himself at a proper distance from it; not coming too close, like the fool, who, on getting scorched, runs away and shivers in solitude, loud in his complaint that the fire burns.\n",
      "- Envy is natural to man and is the enemy of our happiness. Stifle it like an evil thought.\n",
      "- As regards to the envy which we excite in other people, it should be remembered that no form of hatred is so implacable as the hatred that comes from envy. They will be secretly embittered against you; and unless they are restrained by fear, they will always be anxious to let you understand that you are no better than they.\n",
      "- In the case of a misfortune which has already happened and therefore cannot be altered, you should not allow yourself to think that it might have been otherwise; still less that it might have been avoided by such and such means; for reflections of this kind will only add to your distress and make it intolerable.\n",
      "- We should be careful not to be over-anxious on any matter affecting our weal or our woe, not to carry our anxiety to unreasonable or injudicious limits; but coolly and dispassionately to deliberate upon the matter, as though it were an abstract question which did not touch us in particular.\n",
      "- This reigning-in of the imagination which I am recommending, will also forbid us to summon up the memory of the past misfortune, to paint a dark picture of the injustice or harm that has been done to us, the losses we have sustained, the insults, slights and annoyances to which we have been exposed.\n",
      "- It is most important for everyone who is capable of higher and nobler thoughts to keep their mind from being completely engrossed with private affairs and vulgar troubles. Of course for this self-control is necessary.\n",
      "\n",
      "![[Pasted image 20220825121445.png]]\n",
      "![[Pasted image 20220902113633.png]]\n",
      "\n",
      "- Different from [[shiny new models#DALL·E 2]] in that it uses a language only encoder for text rather than the [[shiny new models#CLIP Connecting Text Images]] joint text & image embedding that DALLE-2 uses\n",
      "- Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model.\n",
      "- Imagen uses a large frozen T5-XXL encoder to encode the input text into embeddings. A conditional diffusion model maps the text embedding into a 64×64 image. Imagen further utilizes text-conditional super-resolution [[diffusion model]] to upsample the image 64×64→256×256 and 256×256→1024×1024.\n",
      "- [[2022-10-12-Wednesday]] : Also has a video version now [Imagen Video](https://imagen.research.google/video/)\n",
      "\n",
      "## Glide: towards Photorealistic Image Generation and Editing with Text-guided [[diffusion model]]\n",
      "\n",
      "No blog post surprisingly, [paper](https://arxiv.org/abs/2112.10741)\n",
      "[GitHub - openai/glide-text2im: GLIDE: a diffusion-based text-conditional image synthesis model](https://github.com/openai/glide-text2im)\n",
      "[Google Colab - text2im](https://colab.research.google.com/drive/1ZnZz8j5rjGGHrjBp2yosEKeZ0lGy5blQ?authuser=1) #tinker\n",
      "[Google Colab - inpaint](https://github.com/openai/glide-text2im/blob/main/notebooks/inpaint.ipynb)\n",
      "\n",
      "## Text-to-text Transfer Transformer - T5\n",
      "\n",
      "11B parameters, [Colab](https://colab.sandbox.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb), [Blog](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html), [code](https://goo.gle/t5), and [pre-trained models](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints), Live app for T5 text generation: [Demo – InferKit](https://app.inferkit.com/demo) #tinker\n",
      "\n",
      "- Transfer learning's effectiveness comes from pre-training a model on abundantly-available unlabeled text data with a self-supervised task, such as language modeling or filling in missing words. After that, the model can be fine-tuned on smaller labeled datasets, often resulting in (far) better performance than training on the labeled data alone. The recent success of transfer learning was ignited in 2018 by [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [ULMFiT](https://arxiv.org/abs/1801.06146), [ELMo](https://arxiv.org/abs/1802.05365), and [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), and 2019 saw the development of a huge diversity of new methods like [XLNet](https://arxiv.org/abs/1906.08237), [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html), [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), and [MT-DNN](https://arxiv.org/abs/1901.11504).\n",
      "- With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, [[loss function]], and hyperparameters on *any* NLP task, including [[8-9 - Machine Translation, Seq2Seq and Attention|machine translation]], document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n",
      "- ![[Pasted image 20220825083213.png|800]]\n",
      "- Colossal Clean Crawled Corpus (C4), a cleaned version of Common Crawl that is *two orders of magnitude larger than Wikipedia*. C4 is available through [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/c4)\n",
      "- model architectures: where we found that encoder-decoder models generally outperformed \"decoder-only\" language models;\n",
      "- Numerous creative applications like [Talk To Transformer](https://talktotransformer.com/) and the text-based game [AI Dungeon](https://aidungeon.io/).\n",
      "\n",
      "## Vector-quantized Image Modeling with Improved Vqgan\n",
      "- [blog](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html)\n",
      "- An alternative to [[diffusion model]]. Used as the image tokenizer to build [[shiny new models#Parti]]\n",
      "- Several recent papers have exploited this formulation to dramatically improve image generation results through [pre-quantizing images](https://arxiv.org/abs/1711.00937) into discrete integer codes (represented as natural numbers), and [modeling them](https://arxiv.org/abs/2102.12092) [autoregressively](https://arxiv.org/abs/2012.09841) (i.e., predicting sequences one token at a time). In these approaches, a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) ([[convolutional neural nets|CNN]]) is trained to encode an image into discrete tokens, each corresponding to a small patch of the image. A second stage [[convolutional neural nets|CNN]] or [Transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) is then trained to model the distribution of encoded latent variables. The second stage can also be applied to autoregressively generate an image after the training.\n",
      "![[Pasted image 20220902112742.png]]\n",
      " - Vector-quantized Image Modeling (VIM), which pretrains a [[Transformer]] to predict image tokens autoregressively, where discrete image tokens are produced from improved ViT-VQGAN image quantizers.\n",
      " - The image encoding and decing is done with ViT. The autoregressive [[Transformer]] is  [VQGAN](https://arxiv.org/abs/2012.09841).\n",
      "\n",
      "## Parti\n",
      "\n",
      "[Parti: Pathways Autoregressive Text-to-Image Model](https://parti.research.google/) [Demo](https://media-gen.corp.google.com/)\n",
      "\n",
      "- Autoregressive text-to-image generation model that achieves high-fidelity photorealistic image generation and supports content-rich synthesis involving complex compositions and world knowledge. \n",
      "- Parti and [[shiny new models#ImageGen]] are complementary in exploring two different families of generative models – autoregressive and diffusion, respectively – opening exciting opportunities for combinations of these two powerful models.\n",
      "- Parti treats text-to-image generation as a sequence-to-sequence modeling problem, analogous to [[8-9 - Machine Translation, Seq2Seq and Attention|machine translation]] – this allows it to benefit from advances in large language models, especially capabilities that are unlocked by scaling data and model sizes. In this case, the target outputs are sequences of image tokens instead of text tokens in another language.\n",
      "\n",
      "![[Pasted image 20220902115138.png]]\n",
      "\n",
      "- Uses [[shiny new models#Vector-Quantized Image Modeling with Improved VQGAN]] as image tokenizer\n",
      "\n",
      "## Minerva\n",
      "\n",
      "[[2022-10-03-Monday]]\n",
      "\n",
      "E-Mail From Bill.md\n",
      "# E-mail from Bill\n",
      "\n",
      "![rw-book-cover](https://media.newyorker.com/photos/59092f48c14b3c606c102832/16:9/w_1280,c_limit/940110_r27976.jpg)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[John Seabrook]]\n",
      "Full Title: E-Mail From Bill\n",
      "Category: #articles\n",
      "Document Tags: [ [[wkd]], ]\n",
      "URL: https://www.newyorker.com/magazine/1994/01/10/e-mail-from-bill-gates?utm_source=nl&utm_brand=tny&utm_mailing=TNY_Classics_Sunday_010422&utm_campaign=aud-dev&utm_medium=email&bxid=5f44071f42f18850782a533c&cndid=60476897&hasha=1e5ec51f2cab11499c35b78fbbd85aea&hashb=c015f1131e5effdf3ea5853638fc4f7110311871&hashc=9163bf8c03e4a0995979922f700d62775872afa5fe48f12dda1a925a230d2092&esrc=subscribe-page&mbid=CRMNYR062419\n",
      "Date Highlighted: [[2023-01-07-Saturday]]\n",
      "\n",
      "## Highlights\n",
      "- Operating systems, the most monumental of all software constructions, are like medieval cathedrals: thousands of laborers toil for years on small parts of them, each one working by hand, fashioning zeros and ones into patterns that control switches inside microprocessors, which constitute the brains of a computer. The platonic nature of software—it is invisible, weightless, and odorless; it doesn’t exist in the physical world—determines much of the culture that surrounds it. ([View Highlight](https://read.readwise.io/read/01gp6d2m14tt07g0yz069frr1p))\n",
      "- Just thinking of things as winning is a terrible approach. Success comes from focusing in on what you really like and are good at—not challenging every random thing. ([View Highlight](https://read.readwise.io/read/01gp6e37eqagnxvrk0qsp7haxv))\n",
      "- I think having an ability to deal with things at a very detailed level and a very broad level and synthesize between them is probably the thing that helps me the most. This allows someone to take a deep technical understanding and figure out a business strategy that fits together with it. ([View Highlight](https://read.readwise.io/read/01gp6e4nbzh6hgbavp1m3sqemt))\n",
      "\n",
      "Slate Star Codex and Silicon Valley’s War Against the Media.md\n",
      "# Slate Star Codex and Silicon VALLEY’S War against the Media\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article4.6bc1851654a0.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[Ben Wiseman]]\n",
      "Full Title: Slate Star Codex and Silicon Valley’s War Against the Media\n",
      "Category: #articles\n",
      "URL: https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media\n",
      "Date Highlighted: [[2022-01-22-Saturday]]\n",
      "\n",
      "## Highlights\n",
      "- In the past seven years, S.S.C. has become perhaps the premier public-facing venue of the “rationalist” community, a group of loosely affiliated writers and respondents who first coalesced, in the mid-two-thousands, on sites dedicated to the prospect that, with training and effort, our natural cognitive biases can be overcome. Many of these people work in or around Silicon Valley—as mathematicians, programmers, or computer scientists—and their common interests tend to include artificial intelligence, transhumanism, an appreciation for the subtleties of statistical thinking, and the effective-altruism movement.\n",
      "\n",
      "What I Worked On.md\n",
      "# What I Worked on\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[paulgraham.com]]\n",
      "Full Title: What I Worked On\n",
      "Category: #articles\n",
      "URL: http://paulgraham.com/worked.html\n",
      "Date Highlighted: [[2021-05-14-Friday]]\n",
      "\n",
      "## Highlights\n",
      "- In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored. ([View Highlight](https://instapaper.com/read/1411932130/16381518))\n",
      "- I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that's told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.\n",
      "  What these programs really showed was that there's a subset of natural language that's a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. ([View Highlight](https://instapaper.com/read/1411932130/16381595))\n",
      "- It's scary to think how little I knew about Lisp hacking when I started writing that book. But there's nothing like writing a book about something to help you learn it. ([View Highlight](https://instapaper.com/read/1411932130/16381599))\n",
      "- I learned that it's better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it's depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there's not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it. ([View Highlight](https://instapaper.com/read/1411932130/16381761))\n",
      "- But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it's good to be the \"entry level\" option, even though that will be less prestigious, because if you're not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign. ([View Highlight](https://instapaper.com/read/1411932130/16381778))\n",
      "- One of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them ([View Highlight](https://instapaper.com/read/1411932130/16382014))\n",
      "- Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees you're not on the most common type of wrong one. ([View Highlight](https://instapaper.com/read/1411932130/16382019))\n",
      "- The distinctive thing about Lisp is that its core is a language defined by writing an interpreter in itself. It wasn't originally intended as a programming language in the ordinary sense. It was meant to be a formal model of computation, an alternative to the Turing machine. If you want to write an interpreter for a language in itself, what's the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question. ([View Highlight](https://instapaper.com/read/1411932130/16382680))\n",
      "\n",
      "- Every person is different. And everyone will read your story differently. That’s why analogies can be such a useful tool in [[storytelling]]. They create a shorthand for complicated concepts—a bridge directly to a common experience. ([Location 1654](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=1654))\n",
      "- If you’re going to pour your heart into creating something new, then that thing should be disruptive. It should be bold. It should change something. ([Location 1699](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=1699))\n",
      "- If you do it right, one disruption will fuel the next. One revolution will domino into another. People will laugh at you and tell you it’s ludicrous, but that just means they’re starting to pay attention. You’ve found something worth doing. Keep doing it. ([Location 1839](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=1839))\n",
      "- For any really new product, reliable data will be limited or nonexistent. That doesn’t mean you shouldn’t make a reasonable attempt to gather objective information—the scope of the opportunity, the way people use current solutions, etc. But this information will never be definitive. It won’t make your decisions for you. ([Location 1859](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=1859))\n",
      "- General Magic was flailing and needed a pair of handcuffs. It needed to set a date for launch and hold to it. But that’s always the crisis of V1: when do you launch? You don’t have any customers, you haven’t really told the world what you’re working on. It’s all too easy to just keep working. So you have to force yourself to stop. Construct a deadline and handcuff yourself to it. ([Location 2023](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2023))\n",
      "- We forced as many constraints on ourselves as possible: not too much time, not too much money, and not too many people on the team. That last point is important. ([Location 2044](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2044))\n",
      "- Most people don’t even want to acknowledge that there are opinion-driven decisions.] They demanded to be shown ahead of time that the unit and business economics of the product were sound. But that was impossible. They were asking us to predict the future with near 100 percent confidence. They were asking for proof that a baby could run a marathon before it had even learned to walk. These guys didn’t know much about babies. They knew even less about how to create a new business. ([Location 2282](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2282))\n",
      "- Before you commit to executing on an idea—to starting a company or launching a new product—you should commit to researching it and trying it out first. Practice delayed intuition. ([Location 2416](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2416))\n",
      "- Once you have a really strong “why,” you have the germ of a great idea. But you can’t build a business on a germ. First you have to figure out if this idea is actually strong enough to carry a company. You need to build a business and implementation plan. And you have to understand if it’s something you want to work on for the next five to ten years of your life. ([Location 2433](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2433))\n",
      "- The potential company-destroying problems—and the steps to mitigate them—went on and on. But listing them out, breaking them down, talking honestly about them, that’s what ultimately convinced investors that we really knew what we were getting into. And that we could make it work. Eventually, each of those risks became a rallying cry for the team—instead of avoiding them, we embraced them. We continually said to ourselves, “If it were easy, everyone else would be doing it!” We were innovating. The risks and our ability to solve for them was what set us apart. We would do something nobody else thought possible. ([Location 2520](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2520))\n",
      "- V1 is always completely, utterly terrifying. Always. Big, great, new ideas scare the living crap out of everyone who has them. That’s one of the signs that they’re great. ([Location 2532](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2532))\n",
      "- That’s what you need when you’re going to start a company or start a huge new project—a coach. A mentor. A source of wisdom and aid. Someone who can recognize a brewing problem and warn you about it before it happens. ([Location 2632](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2632))\n",
      "- If you want to start a company, if you want to start anything, to create something new, then you need to be ready to push for greatness. And greatness doesn’t come from nothing. You have to prepare. You have to know where you’re headed and remember where you came from. You have to make hard decisions and be the mission-driven “asshole.” ([Location 2673](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2673))\n",
      "- So do not lose your focus. Do not think you can serve two masters. No matter what you’re building, you can never forget who you’re building it for. You can only have one customer. Choose wisely. ([Location 2911](https://readwise.io/to_kindle?action=open&asin=B09BNJ6GBV&location=2911))\n",
      "\n",
      "103 Bits of Advice I Wish I Had Known.md\n",
      "# 103 Bits of Advice I Wish I Had Known\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article4.6bc1851654a0.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[The Technium]]\n",
      "Full Title: 103 Bits of Advice I Wish I Had Known\n",
      "Category: #articles\n",
      "URL: https://kk.org/thetechnium/103-bits-of-advice-i-wish-i-had-known/\n",
      "Date Highlighted: [[2022-05-06-Friday]]\n",
      "\n",
      "## Highlights\n",
      "- Productivity is often a distraction. Don’t aim for better ways to get through your tasks as quickly as possible, rather aim for better tasks that you never want to stop doing.\n",
      "- What you do on your bad days matters more than what you do on your good days.\n",
      "- Focus on directions rather than destinations. Who knows their destiny? But maintain the right direction and you’ll arrive at where you want to go.\n",
      "- Speak confidently as if you are right, but listen carefully as if you are wrong.\n",
      "- When introduced to someone make eye contact and count to 4. You’ll both remember each other.\n",
      "- Prescription for popular success: do something strange. Make a habit of your weird.\n",
      "- No one is as impressed with your possessions as you are.\n",
      "- The only productive way to answer “what should I do now?” is to first tackle the question of “who should I become?”\n",
      "- When you forgive others, they may not notice, but you will heal. Forgiveness is not something we do for others; it is a gift to ourselves.\n",
      "- Efficiency is highly overrated; Goofing off is highly underrated. Regularly scheduled sabbaths, sabbaticals, vacations, breaks, aimless walks and time off are essential for top performance of any kind. The best work ethic requires a good rest ethic.\n",
      "- Ask anyone you admire: Their lucky breaks happened on a detour from their main goal. So embrace detours. Life is not a straight line for anyone.\n",
      "\n",
      "2022-8-7 arXiv Roundup Adam and Sharpness, Recursive Self-Improvement for Coding, Training and Model Tweaks.md\n",
      "# 2022-8-7 Arxiv Roundup: Adam and Sharpness, Recursive Self-improvement for Coding, Training and Model Tweaks\n",
      "\n",
      "![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png)\n",
      "\n",
      "## Metadata\n",
      "\n",
      "Author: [[dblalock.substack.com]]\n",
      "Full Title: 2022-8-7 arXiv Roundup: Adam and Sharpness, Recursive Self-Improvement for Coding, Training and Model Tweaks\n",
      "Category: #articles\n",
      "URL: https://dblalock.substack.com/p/2022-8-7-arxiv-roundup-adam-and-sharpness?utm_medium=email\n",
      "Date Highlighted: [[2022-09-21-Wednesday]]\n",
      "\n",
      "## Highlights\n",
      "- Language Models Can Teach Themselves to Program BetterTo teach models to program, you used to give them a natural language prompt. But recent work has shown that you can instead just show them a unit test and tell them to generate a program that satisfies it (a “programming puzzle”). This is way nicer because it’s simpler and you can just run the code to see if it works.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fm = retrieve(\"xcs224\", 0.70)\n",
    "\n",
    "print(len(fm))\n",
    "for fms in fm:\n",
    "    print(fms['metadata']['note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'header'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# then we complete the context-infused query\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#ans = complete(query_with_contexts)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m filtered_matches:\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx[\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m --- \u001b[39m\u001b[39m{\u001b[39;00mx[\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m --- \u001b[39m\u001b[39m{\u001b[39;00mx[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'header'"
     ]
    }
   ],
   "source": [
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts, filtered_matches = retrieve_context(\"what is machine learning?\", 0.7)\n",
    "# then we complete the context-infused query\n",
    "#ans = complete(query_with_contexts)\n",
    "for x in filtered_matches:\n",
    "    print(f\"{x['metadata']['file']} --- {x['metadata']['header']} --- {x['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"---foobar\\n\\nammar\\nhusain\"\n",
    "\n",
    "my_list = [x for x in a.split(\"\\n\") if x != \"\"]\n",
    "print(my_list)\n",
    "\n",
    "if re.search(r'\\w', a):\n",
    "    print(\"The string starts is an alphanumeric character\")\n",
    "else:\n",
    "    print(\"The string does not start with an alphanumeric character\")\n",
    "\n",
    "print(re.search(r'\\w', a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "\n",
    "if a:\n",
    "    print(\"foo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
